{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/albertomonedero/fashioncnn?scriptVersionId=132148689\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# SISTEMA DE RECOMENDACIÓN DE ARTÍCULOS DE MODA","metadata":{}},{"cell_type":"code","source":"from numpy import loadtxt\nimport pandas as pd\nfrom PIL import Image\nfrom fastcore.all import *\nfrom torch.utils.data import Dataset ,DataLoader\nimport torchvision.transforms as transforms\nfrom tqdm.notebook import tqdm\nimport torch\nimport numpy\nfrom torch import nn\nimport torchmetrics as metrics\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt \n\n\ndataFrame = pd.read_csv('../input/fashion-product-images-small/styles.csv',on_bad_lines='skip').dropna()\ndataFrame\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(sys.version)\nprint(numpy.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preparación de los datos de entrada (CSV)**\nComo quiero clasificar las prendas únicamente por género y estilo, creo un nuevo csv a mi gusto, quedándome sólo con dos columnas:\n* image = nombre del archivo de imagen, ej ->\"15970.jpg\".\n* target = columna objetivo,representada por un entero que equivale a un usage+gender concreto\n            ej -> \"0\", que sería CasualMen.\n\nAdemás debo eliminar ciertas entradas del csv original erróneas, ya que existen algunas entradas con ids que no se corresponden a ninguna imágen de la carpeta de imágenes.\n","metadata":{}},{"cell_type":"code","source":"#creo la columna image con id+.jpg y la columna target con usage+gender\n\ndef getImageName(id): return '{name}.jpg'.format(name = id)\n\ndataFrame['image'] = dataFrame['id'].map(getImageName)\ndataFrame['target'] =dataFrame['usage']+dataFrame['gender']\n\ndataFrame.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nos quedamos solo con las columnas image y target\nds = dataFrame.filter(['image', 'target'])\nds.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tenemos 25 clases objetivo en nuestro dataframe\ntargets=ds.target.unique()\nCLASSES=len(targets)\nprint(CLASSES)\nprint(targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creo un diccionario para mapear targets con enteros\n#Sustituyo en el dataFrame el nombre de los targets por el número correspondiente en dicho diccionario\ni=0\nclassDic={}\nfor t in targets:\n    ds['target']=ds['target'].replace(t,i)\n    classDic[i]=t\n    i+=1\nprint(ds)\nprint(classDic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Procedo a eliminar las filas con ids de imágenes que no existen en la carpeta de imágenes\nimages=ds.image.unique()\nimagenes_existentes=os.listdir('../input/fashion-product-images-small/images')\nfor i in images:\n    if(i not in imagenes_existentes):\n        ds=ds.drop(ds[ds['image']==i].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#guardo mi nuevo dataframe en un csv\nds.to_csv('custom.csv',index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creación del dataset personalizado","metadata":{}},{"cell_type":"code","source":"transformaciones = transforms.Compose([transforms.Resize((64,64)),\n                                      transforms.ToTensor(), #0 - 255--->0 - 1\n                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) #0 - 1--->-1 - 1\n                                     ])\nbatch_size = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform):\n        self.annotations = pd.read_csv(csv_file, on_bad_lines='skip')\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n        image = self.transform(image)\n\n        return (image, y_label)\n\n\n#Instanciamos el dataset con nuestro csv personalizado, las imágenes \n#y las transformaciones que se le aplican a las mismas \ndataset = CustomDataset(\n    csv_file=\"./custom.csv\",\n    root_dir=\"../input/fashion-product-images-small/images/\",\n    transform=transformaciones,\n)\nprint(len(dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dividimos el dataset en 3:entrenamiento, validación y test, y creamos los dataloaders\ntrain_set,val_set, test_set = torch.utils.data.random_split(dataset, [30000, 9072,5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, num_workers=2)\nval_loader = DataLoader(dataset=val_set, batch_size=batch_size,  num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Primera Fase: CNN para clasificar imágenes por género y estilo.\n","metadata":{}},{"cell_type":"markdown","source":"# Creación del Modelo CNN","metadata":{}},{"cell_type":"code","source":"class CNNModel(pl.LightningModule):\n    def __init__(self):\n        #shape=[batchsize,canalesentrada,alto,ancho]\n        #entrada [64, 3, 64, 64])\n        super().__init__()\n    #EXTRACCIÓN DE CARACTERÍSTICAS\n        #1 BLOQUE CONV\n        self.cnv = nn.Conv2d(3,40,5,2)#[64, 40, 30, 30]\n        self.rel = nn.ReLU()                    \n        self.bn = nn.BatchNorm2d(40)  \n        self.mxpool = nn.MaxPool2d(2)#[64, 40, 15, 15]\n        #2 BLOQUE CONV\n        self.cnv2 = nn.Conv2d(40,55,5,2)#[64, 55, 6, 6])\n        self.rel2 = nn.ReLU()                    \n        self.bn2 = nn.BatchNorm2d(55)  \n        self.mxpool2 = nn.MaxPool2d(2)#[64, 55, 3, 3]\n        \n        \n    #CARACTERIZACIÓN\n        self.flat = nn.Flatten()#[64,55x3x3=495]\n        self.fc1 = nn.Linear(495,495)\n        self.fc2 = nn.Linear(495,300)\n        self.fc3 = nn.Linear(300,CLASSES)\n        self.softmax = nn.Softmax()\n        self.accuracy = metrics.Accuracy(task='multiclass',num_classes=25) #predicciones correctas/total de predicciones\n\n    def forward(self,x):\n       # print('antes conv',x.shape)\n        out = self.cnv(x)\n        #print('despues conv',out.shape)\n        out = self.rel(out)\n       # print('despues relu',out.shape)\n        out = self.bn(out)\n        #print('despues batchnorm',out.shape)\n        out = self.mxpool(out)\n        #rint('despues maxpool',out.shape)\n        \n        \n        out = self.cnv2(out)\n        #print('despues conv2',out.shape)\n        out = self.rel2(out)\n        #print('despues relu2',out.shape)\n        out = self.bn2(out)\n        #print('despues batchnorm2',out.shape)\n        out = self.mxpool2(out)\n       # print('despues maxpool2',out.shape)\n        \n        out = self.flat(out)\n       # print('despues flat',out.shape)\n        out = self.rel(self.fc1(out))\n        out = self.rel(self.fc2(out))   \n        out = self.fc3(out)\n        return out\n\n    def loss_fn(self,out,target):\n        return nn.CrossEntropyLoss()(out.view(-1,CLASSES),target)\n    \n    def configure_optimizers(self):\n        LR = 1e-3\n        optimizer = torch.optim.AdamW(self.parameters(),lr=LR)\n        return optimizer\n    \n    def predict(self, x):\n        with torch.no_grad():\n            out = self(x)\n            out=nn.Softmax(-1)(out)\n            return torch.argmax(out, axis=1)\n\n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        imgs = x.view(-1,3,64,64)\n        labels = y.view(-1)\n        out = self(imgs)\n        loss = self.loss_fn(out,labels)\n        out = nn.Softmax(-1)(out)\n        logits = torch.argmax(out,dim=1)\n        accu = self.accuracy(logits, labels)\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_acc', accu, prog_bar=True)\n        return loss       \n\n    def validation_step(self,batch,batch_idx):\n        x,y = batch\n        imgs = x.view(-1,3,64,64)\n        labels = y.view(-1)\n        out = self(imgs)\n        loss = self.loss_fn(out,labels)\n        out = nn.Softmax(-1)(out) \n        logits = torch.argmax(out,dim=1)\n        accu = self.accuracy(logits, labels)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', accu, prog_bar=True)\n        return loss, accu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod = CNNModel().to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exportamos el modelo en formato ONNX para hacer uso de la aplicación Netron, que permite visualizar la arquitectura de un modelo de deep learning.\n","metadata":{}},{"cell_type":"code","source":"import torch.onnx\n\ninput_names = [\"input\"]\noutput_names = [\"output\"]\n\n\n\nbatch=next(iter(test_loader)) #cogemos 1 lote, batch=[[imgs],[labels]]\nimgs, labels= batch[0],batch[1]\n\n\n\n# Exportamos el modelo a ONNX\ntorch.onnx.export(mod, imgs.to(device), \"model.onnx\", input_names=input_names, output_names=output_names, opset_version=11)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Entrenamiento del modelo\n","metadata":{}},{"cell_type":"code","source":"mod.train()\ntrainer = pl.Trainer(accelerator='gpu',devices=1,\n                     max_epochs=4\n                    \n)\ntrainer.fit(mod,train_loader,val_loader) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Guardamos el estado del modelo entrenado\nstate_dict = mod.state_dict()\ntorch.save(state_dict, \"prueba7.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pruebas: Calcular el rendimiento con datos de Test, obtener matriz de confusión y probar a predecir una imágen\n","metadata":{}},{"cell_type":"code","source":"#Cargo mi modelo ya entrenado\nstate = torch.load(\"../input/modelo6/prueba6.pth\") #por ahora modelo 6 es el mejor\n\nnew_model =CNNModel()\nnew_model.load_state_dict(state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_set),len(test_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cálculo de rendimiento","metadata":{}},{"cell_type":"code","source":"    #Calculamos la acc y loss del modelo \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        total_loss=0\n        new_model.eval()\n        for data, target in test_loader: #vamos iterando los distintos lotes del test_loader\n            images = data\n            labels =target\n            outputs = new_model(images)\n            total_loss+=new_model.loss_fn(outputs,labels)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n                \n\n            accuracy = correct / total\n\n        print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n        print('loss',total_loss/len(test_loader))  #divido la perdida en cada lote por el numero de lotes\n \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(targets)\nprint(classDic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Matriz de confusión","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\n\n# Crear listas de etiquetas verdaderas y predichas\nall_labels = list(range(len(targets)))\npredicted_labels = []\ntrue_labels = []\nfor data, target in test_loader:\n    images = data\n    labels = target\n    outputs = new_model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    predicted_labels.extend(predicted.tolist())\n    true_labels.extend(labels.tolist())\n\n# Agregar las etiquetas faltantes a las listas correspondientes\nmissing_labels = list(set(all_labels) - set(predicted_labels))\npredicted_labels += missing_labels\ntrue_labels += missing_labels\n\n# Calcular la matriz de confusión\ncm = confusion_matrix(true_labels, predicted_labels)\n\n# Imprimir la matriz de confusión\ndf_cm = pd.DataFrame(cm, index=targets, columns=targets)\nplt.figure(figsize=(30, 25))\nsn.set(font_scale=1.8)\nsn.heatmap(df_cm, annot=True, cmap=\"Blues\", fmt='g', linewidths=1.5, square=True)\nplt.xlabel('Predicho')\nplt.ylabel('Verdadero')\nplt.title('Matriz de confusión', fontsize=18)\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Obtenemos las metricas de la matriz de confusion\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(true_labels, predicted_labels, target_names=targets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ejemplo de predicción","metadata":{}},{"cell_type":"code","source":"#PROBAMOS A PREDECIR 1 DE LAS IMAGENES DEL TEST_LOADER\n\n\nbatch=next(iter(test_loader)) #cogemos 1 lote, batch=[[imgs],[labels]]\nimgs, labels= batch[0],batch[1]\nim=imgs[1]\nlabel=labels[1]\nplt.imshow(im.numpy()[0], cmap='gray')\nprint(\"valor esperado:\",classDic[label.item()])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = new_model(im.unsqueeze(0))\n_, predicted = torch.max(output, 1)\netiqueta =classDic[predicted.item()]\nprint('Valor predicho: ',etiqueta )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lo mismo pero probando la función predict del modelo\nprediction = new_model.predict(im.unsqueeze(0))\n\netiqueta2=classDic[prediction.item()]\nprint('Valor predicho: ',etiqueta2 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segunda Fase: Tras clasificar una prenda, obtener los candidatos a recomendar (aquellos con la misma etiqueta) y devolver los N candidatos más parecidos.\n","metadata":{}},{"cell_type":"markdown","source":"Clasifico una prenda","metadata":{}},{"cell_type":"code","source":"#imagen a clasificar\nimg = Image.open('../input/fashion-product-images-small/images/10080.jpg')\nimg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clasificar dicha imagen\nt_img = transformaciones(img).unsqueeze(0)\noutput = new_model(t_img)\n_, predicted = torch.max(output, 1)\netiqueta =classDic[predicted.item()]\nprint('Valor predicho: ',etiqueta )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lo mismo pero usando la función prediction definida en el modelo\nt_img = transformaciones(img).unsqueeze(0)\nprediction = new_model.predict(t_img)\n\netiqueta2=classDic[prediction.item()]\nprint('Valor predicho: ',etiqueta2 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A continuación, devuelvo los posibles candidatos ( en este caso, prendas deportivas de hombre)","metadata":{}},{"cell_type":"code","source":"print(ds)\nprint(classDic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creo una condición para obtener candidatas del dataframe\ncondicion_etiqueta = ds['target'] == predicted.item()\ncondicion_etiqueta.head()\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidatas=ds[condicion_etiqueta]\nprint(candidatas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Función para devolver las N candidatas más parecidas a una imagen dada\n\nUso la librería img2vect de pytorch que usa modelos pre-entrenados (en mi caso resnet-18) para obtener la representación vectorial de las imágenes, y después calcular la similitud usando como función la similitud coseno.","metadata":{}},{"cell_type":"code","source":"pip install img2vec-pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from img2vec_pytorch import Img2Vec\nimg2vec = Img2Vec(cuda=True, model='resnet-18')\ncos = nn.CosineSimilarity(eps=1e-6)\nimagenes=candidatas.image.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"esta función no devuelve las recomendaciones ordenadas directamente, habría que ordenarlas después","metadata":{}},{"cell_type":"code","source":"\ndef N_mas_parecidas(imagen,n,imagenes):\n   \n    vec1 = img2vec.get_vec(imagen, tensor=True).reshape(512) #representación vectorial imagen de entrada\n    dic={}#diccionario con imagenes y similitudes\n    similitudes=[]\n    recomendadas={}\n    for im in imagenes: #cargo el diccionario con cada imagen como clave y la similitud como valor\n                        #y obtengo una lista con las n mayores similitudes, ordenada de mayor a menor\n        candidata = Image.open('../input/fashion-product-images-small/images/'+im)       \n        vec2 = img2vec.get_vec(candidata.convert('RGB'), tensor=True).reshape(512)\n        cos_sim = cos(vec1.unsqueeze(0),vec2.unsqueeze(0))\n        if(cos_sim!=1): #no quiero devolver la propia imagen cono recomendación\n            dic[im]=cos_sim\n            similitudes.append(cos_sim)\n    n_mayor_similitud=sorted(list(set(similitudes)), reverse=True)[:n]\n    for im in imagenes:\n        if(dic.get(im) in n_mayor_similitud):\n            recomendadas[im]=dic.get(im)\n            \n    return recomendadas ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Versión optimizada utilizando librería heapq para algoritmo de montículo**","metadata":{}},{"cell_type":"markdown","source":"Esta versión si que devuelve ya las recomendaciones ordenadas de mayor a menor similitud","metadata":{}},{"cell_type":"code","source":"import heapq\n\ndef N_mas_parecidas_optimizada(imagen,n,imagenes):\n    \n    vec1 = img2vec.get_vec(imagen, tensor=True).reshape(512) #representación vectorial imagen de entrada\n   \n    heap=[] #lista de tuplas (similitud,imagen)\n\n    for im in imagenes: \n        candidata = Image.open('../input/fashion-product-images-small/images/'+im)       \n        vec2 = img2vec.get_vec(candidata.convert('RGB'), tensor=True).reshape(512)\n        cos_sim = cos(vec1.unsqueeze(0),vec2.unsqueeze(0))\n        if(cos_sim<1): #no quiero devolver la propia imagen cono recomendación\n            \n            heapq.heappush(heap, (round(cos_sim.item(),7), im))\n    \n    \n    return heapq.nlargest(n, heap) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probamos las funciones","metadata":{}},{"cell_type":"code","source":"dic=N_mas_parecidas(img,5,imagenes)\n\nsimilares=[]\nfor nombre in dic.keys():\n     similares.append(nombre)\nprint(dic)\nprint(similares)\nrec1 = Image.open('../input/fashion-product-images-small/images/'+similares[0]) \nrec2 = Image.open('../input/fashion-product-images-small/images/'+similares[1]) \nrec3 = Image.open('../input/fashion-product-images-small/images/'+similares[2]) \nrec4 = Image.open('../input/fashion-product-images-small/images/'+similares[3]) \nrec5 = Image.open('../input/fashion-product-images-small/images/'+similares[4]) \n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"imagen clasificada:","metadata":{}},{"cell_type":"code","source":"img  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"recomendaciones:","metadata":{}},{"cell_type":"code","source":"rec1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lista=N_mas_parecidas_optimizada(img,5,imagenes)\n\nsimilares=[]\nfor tupla in lista:\n     similares.append(tupla[1])\nprint(lista)\nprint(similares)\nrec1 = Image.open('../input/fashion-product-images-small/images/'+similares[0]) \nrec2 = Image.open('../input/fashion-product-images-small/images/'+similares[1]) \nrec3 = Image.open('../input/fashion-product-images-small/images/'+similares[2]) \nrec4 = Image.open('../input/fashion-product-images-small/images/'+similares[3]) \nrec5 = Image.open('../input/fashion-product-images-small/images/'+similares[4]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COMPARACIÓN DEL RENDIMIENTO DE MI CNN CLASIFICADORA CON OTRAS PRE-ENTRENADAS\n","metadata":{}},{"cell_type":"markdown","source":"Para usar los modelos pre-entrenados para clasificar un dataset propio hay que re-entrenarlos con mi propio dataset. Para hacer esto, debo cambiar la última capa de clasificación para que tenga un número de salidas igual al número de clases en mi dataset. Luego, debo entrenar los modelos con mi dataset de forma que se ajusten a las características de mis imágenes y las clases que intento clasificar. Finalmente, puedo evaluar la precisión del modelo en mis datos de prueba.","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" La normalización para imagenes de ImageNet suele ser similar para muchos modelos preentrenados, incluyendo ResNet18, VGG16 y DenseNet121. La normalización típica consiste en restar la media de los valores de los canales RGB y dividir por la desviación estándar. Los valores concretos pueden variar, pero a menudo se usan [0.485, 0.456, 0.406] y [0.229, 0.224, 0.225] como valores de media y desviación estándar, respectivamente.\nCreo un dataset que se normalizará usando las transformaciones esperadas por modelos preentrenados como ResNet, VGG y DenseNet. \n ","metadata":{}},{"cell_type":"code","source":"\ntransformaciones_imagenet = transforms.Compose([transforms.Resize((224,224)),\n                                      transforms.ToTensor(), \n                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n                                     ])\nImageNetTransformsdataset =CustomDataset(\n    csv_file=\"./custom.csv\",\n    root_dir=\"../input/fashion-product-images-small/images/\",\n    transform=transformaciones_imagenet,\n)\n#Dividimos el dataset en 2:entrenamiento y test, y creamos los dataloaders\ntrain_set_imageNet, test_set_imageNet = torch.utils.data.random_split(dataset, [30000, 14072])\ntrain_loader_imageNet = DataLoader(dataset=train_set_imageNet, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\ntest_loader_imageNet = DataLoader(dataset=test_set_imageNet, batch_size=batch_size, num_workers=2)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_set_imageNet),len(train_loader_imageNet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning con resnet-18","metadata":{}},{"cell_type":"markdown","source":"Para implementar las funciones de entrenamiento a mano, me he basado en esta guía: ","metadata":{}},{"cell_type":"markdown","source":" [tutorial transfer learning](https://medium.com/nerd-for-tech/image-classification-using-transfer-learning-pytorch-resnet18-32b642148cbe)","metadata":{}},{"cell_type":"code","source":"\n# Definir el modelo ResNet-18\nmodel = models.resnet18(pretrained=True)\n\n# Modificamos la salida del modelo para adaptarse a 25 clases\nnum_ftrs = model.fc.in_features  #obtenemos el numero de neuronas de entrada de la capa de salida en resnet\nmodel.fc = nn.Linear(num_ftrs, 25) #sustituimos la capa final por una con 25 neuronas\nmodel = model.to(device)\n\n\n# Definir la función de pérdida y el optimizador (utilizo las mismas que en mi modelo)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 11\nfor epoch in range(num_epochs): #(loop for every epoch)\n    print(\"Epoch {} running\".format(epoch)) #(printing message)\n    \"\"\" Training Phase \"\"\"\n    model.train()    #(training model)\n    running_loss = 0.   #(set loss 0)\n    running_corrects = 0 \n    # load a batch data of images\n    for i, (inputs, labels) in enumerate(train_loader_imageNet):\n        inputs = inputs.to(device)\n        labels = labels.to(device) \n        # forward inputs and get output\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        # get loss value and update the network weights\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    epoch_loss = running_loss / len(train_set_imageNet)  #divido por el tamaño del dataset, ya que una época recorre todo el dataset\n    epoch_acc = running_corrects / len(train_set_imageNet) * 100.\n    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch, epoch_loss, epoch_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"resnet18.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nresnet18_model = models.resnet18(pretrained=True)  \nnum_features = resnet18_model.fc.in_features \nresnet18_model.fc = nn.Linear(num_features, 25)\nresnet18_model.load_state_dict(torch.load(\"resnet18.pth\"))\nresnet18_model.to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PROBAMOS A PREDECIR 1 DE LAS IMAGENES DEL TEST_LOADER\n\n\nbatch=next(iter(test_loader_imageNet)) #cogemos 1 lote, batch=[[imgs],[labels]]\nimgs, labels= batch[0],batch[1]\nim=imgs[1]\nlabel=labels[1]\nplt.imshow(im.numpy()[0], cmap='gray')\nprint(\"valor esperado:\",classDic[label.item()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = resnet18_model(im.unsqueeze(0).to(device))\n_, predicted = torch.max(output, 1)\netiqueta =classDic[predicted.item()]\nprint('Valor predicho: ',etiqueta )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport time\n\n##Testing\n\nstart_time = time.time()\ncriterion = nn.CrossEntropyLoss()\n\n\n#Calculamos la acc y loss del modelo \nwith torch.no_grad():\n        correct = 0\n        total = 0\n        total_loss=0\n        resnet18_model.eval()\n        for data, target in test_loader_imageNet: #vamos iterando los distintos lotes del test_loader\n            images = data\n            labels =target\n            images = images.to('cuda')\n            labels = labels.to('cuda')\n            outputs = resnet18_model(images)\n            total_loss += criterion(outputs,labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n                \n\n            accuracy = correct / total\n\n        print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n        print('loss',total_loss/len(test_loader_imageNet))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning con vgg16","metadata":{}},{"cell_type":"code","source":"# Definir el modelo ResNet-18\nmodel = models.vgg16(pretrained=True)\n#vgg16 tiene 7 capas lineales\nnum_ftrs = model.classifier[6].in_features #obtenemos el numero de neuronas de entrada de la capa de salida en vgg\nmodel.classifier[6] = nn.Linear(num_ftrs, 25) #sustituimos la capa final por una con 25 neuronas\nmodel = model.to(device)\n\n# Definir la función de pérdida y el optimizador (utilizo las mismas que en mi modelo)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 10\nfor epoch in range(num_epochs): #(loop for every epoch)\n    print(\"Epoch {} running\".format(epoch)) #(printing message)\n    \"\"\" Training Phase \"\"\"\n    model.train()    #(training model)\n    running_loss = 0.   #(set loss 0)\n    running_corrects = 0 \n    # load a batch data of images\n    for i, (inputs, labels) in enumerate(train_loader_imageNet):\n        inputs = inputs.to(device)\n        labels = labels.to(device) \n        # forward inputs and get output\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        # get loss value and update the network weights\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    epoch_loss = running_loss / len(train_set_imageNet)  #divido por el tamaño del dataset, ya que una época recorre todo el dataset\n    epoch_acc = running_corrects / len(train_set_imageNet) * 100.\n    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch, epoch_loss, epoch_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"vgg16.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model = models.vgg16(pretrained=True)  \nnum_features = vgg16_model.classifier[6].in_features \nvgg16_model.classifier[6] = nn.Linear(num_features, 25)\nvgg16_model.load_state_dict(torch.load(\"vgg16.pth\"))\nvgg16_model.to(device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torchvision\n\n\n##Testing\n\nstart_time = time.time()\ncriterion = nn.CrossEntropyLoss()\n\n\n#Calculamos la acc y loss del modelo \nwith torch.no_grad():\n        correct = 0\n        total = 0\n        total_loss=0\n        vgg16_model.eval()\n        for data, target in test_loader_imageNet: #vamos iterando los distintos lotes del test_loader\n            images = data\n            labels =target\n            images = images.to('cuda')\n            labels = labels.to('cuda')\n            outputs = vgg16_model(images)\n            total_loss += criterion(outputs,labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n                \n\n            accuracy = correct / total\n\n        print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n        print('loss',total_loss/len(test_loader_imageNet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning con densenet121","metadata":{}},{"cell_type":"code","source":"# Definir el modelo DenseNet-121\nmodel = models.densenet121(pretrained=True)\n\n# Modificamos la salida del modelo para adaptarse a 25 clases\nnum_ftrs = model.classifier.in_features #obtenemos el numero de neuronas de entrada de la capa de salida en DenseNet\nmodel.classifier = nn.Linear(num_ftrs, 25) #sustituimos la capa final por una con 25 neuronas\nmodel = model.to(device)\n# Definir la función de pérdida y el optimizador (utilizo las mismas que en mi modelo)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\nfor epoch in range(num_epochs): #(loop for every epoch)\n    print(\"Epoch {} running\".format(epoch)) #(printing message)\n    \"\"\" Training Phase \"\"\"\n    model.train()    #(training model)\n    running_loss = 0.   #(set loss 0)\n    running_corrects = 0 \n    # load a batch data of images\n    for i, (inputs, labels) in enumerate(train_loader_imageNet):\n        inputs = inputs.to(device)\n        labels = labels.to(device) \n        # forward inputs and get output\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        # get loss value and update the network weights\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    epoch_loss = running_loss / len(train_set_imageNet)  #divido por el tamaño del dataset, ya que una época recorre todo el dataset\n    epoch_acc = running_corrects / len(train_set_imageNet) * 100.\n    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch, epoch_loss, epoch_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"densenet121.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndensenet121_model = models.densenet121(pretrained=True)\nnum_ftrs = densenet121_model.classifier.in_features \ndensenet121_model.classifier = nn.Linear(num_ftrs, 25) \n\ndensenet121_model.load_state_dict(torch.load(\"densenet121.pth\"))\ndensenet121_model = densenet121_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Calculamos la acc y loss del modelo \nwith torch.no_grad():\n        correct = 0\n        total = 0\n        total_loss=0\n        densenet121_model.eval()\n        for data, target in test_loader_imageNet: #vamos iterando los distintos lotes del test_loader\n            images = data\n            labels =target\n            images = images.to('cuda')\n            labels = labels.to('cuda')\n            outputs = densenet121_model(images)\n            total_loss += criterion(outputs,labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n                \n\n            accuracy = correct / total\n\n        print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n        print('loss',total_loss/len(test_loader_imageNet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PRUEBAS FINE-TUNING CONGELANDO LAS PRIMERAS CAPAS\n# ","metadata":{}},{"cell_type":"markdown","source":"# Resnet18","metadata":{}},{"cell_type":"markdown","source":"Pruebo a congelar todas las secuencias de capas convolucionales excepto la última (layer4). Por lo que solo actualizaría los pesos de las capas de la última secuencia de convoluciones y de la capa de clasificación. Las capas convolucionales en los modelos de redes neuronales suelen ser agrupadas en secuencias lógicas, y en ResNet18 estas secuencias se nombran como \"layer1\", \"layer2\", \"layer3\" y \"layer4\". Cada una de estas secuencias tiene varias capas convolucionales dentro de ellas.","metadata":{}},{"cell_type":"code","source":"\n# Definir el modelo ResNet-18\nmodel = models.resnet18(pretrained=True)\n\n# Congelar las primeras 15 capas convolucionales\nfor i, param in model.named_parameters():\n    if 'layer4' not in i and 'fc' not in i:\n        param.requires_grad = False\n        \n# Modificamos la salida del modelo para adaptarse a 25 clases\nnum_ftrs = model.fc.in_features  #obtenemos el numero de neuronas de entrada de la capa de salida en resnet\nmodel.fc = nn.Linear(num_ftrs, 25) #sustituimos la capa final por una con 25 neuronas\nmodel = model.to(device)\n\n\n# Definir la función de pérdida y el optimizador (utilizo las mismas que en mi modelo)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 20\nfor epoch in range(num_epochs): #(loop for every epoch)\n    print(\"Epoch {} running\".format(epoch)) #(printing message)\n    \"\"\" Training Phase \"\"\"\n    model.train()    #(training model)\n    running_loss = 0.   #(set loss 0)\n    running_corrects = 0 \n    # load a batch data of images\n    for i, (inputs, labels) in enumerate(train_loader_imageNet):\n        inputs = inputs.to(device)\n        labels = labels.to(device) \n        # forward inputs and get output\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        # get loss value and update the network weights\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    epoch_loss = running_loss / len(train_set_imageNet)  #divido por el tamaño del dataset, ya que una época recorre todo el dataset\n    epoch_acc = running_corrects / len(train_set_imageNet) * 100.\n    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch, epoch_loss, epoch_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"resnet18-2.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nresnet18_model = models.resnet18(pretrained=True)  \n\n# Congelar las primeras 15 capas convolucionales\nfor i, param in resnet18_model.named_parameters():\n    if 'layer4' not in i and 'fc' not in i:\n        param.requires_grad = False\n        \nnum_features = resnet18_model.fc.in_features \nresnet18_model.fc = nn.Linear(num_features, 25)\nresnet18_model.load_state_dict(torch.load(\"resnet18-2.pth\"))\nresnet18_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport time\n\n##Testing\n\nstart_time = time.time()\ncriterion = nn.CrossEntropyLoss()\n\n\n#Calculamos la acc y loss del modelo \nwith torch.no_grad():\n        correct = 0\n        total = 0\n        total_loss=0\n        resnet18_model.eval()\n        for data, target in test_loader_imageNet: #vamos iterando los distintos lotes del test_loader\n            images = data\n            labels =target\n            images = images.to('cuda')\n            labels = labels.to('cuda')\n            outputs = resnet18_model(images)\n            total_loss += criterion(outputs,labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n                \n\n            accuracy = correct / total\n\n        print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n        print('loss',total_loss/len(test_loader_imageNet))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Análisis de las etiquetas del dataset**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"A continuación se analiza la representación en el dataset de las distintas clases obtenidas al combinar la etiqueta de género con el resto del dataset, para intentar solucionar el segundo problema explicado en la memoria. El objetivo es el de obtener el porcentaje de muestras del dataset obtenida para cada clase en cada combinación de etiquetas.\n","metadata":{}},{"cell_type":"markdown","source":"# Representación de las etiquetas","metadata":{}},{"cell_type":"code","source":"usage= dataFrame.usage.unique()\n\nporcentajesUsage={}\nfor etiqueta in usage:\n    perc = (((dataFrame['usage'] == etiqueta)).sum() / dataFrame.shape[0]) * 100\n    porcentajesUsage[etiqueta] = round(perc,4)\nprint(porcentajesUsage)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masterCategory= dataFrame.masterCategory.unique()\n\nporcentajesMasterCategory={}\nfor etiqueta in masterCategory:\n    perc = (((dataFrame['masterCategory'] == etiqueta)).sum() / dataFrame.shape[0]) * 100\n    porcentajesMasterCategory[etiqueta] = round(perc,4)\nprint(porcentajesMasterCategory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subCategory= dataFrame.subCategory.unique()\n\nporcentajesSubCategory={}\nfor etiqueta in subCategory:\n    perc = (((dataFrame['subCategory'] == etiqueta)).sum() / dataFrame.shape[0]) * 100\n    porcentajesSubCategory[etiqueta] = round(perc,4)\nprint(porcentajesSubCategory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articleType= dataFrame.articleType.unique()\n\nporcentajesArticleType={}\nfor etiqueta in articleType:\n    perc = (((dataFrame['articleType'] == etiqueta)).sum() / dataFrame.shape[0]) * 100\n    porcentajesArticleType[etiqueta] = round(perc,4)\nprint(porcentajesArticleType)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Representación de las clases fruto de combinar las distintas etiquetas con la etiqueta género\n# ","metadata":{}},{"cell_type":"markdown","source":"**Usage**","metadata":{}},{"cell_type":"code","source":"#creo la columna image con id+.jpg y la columna target con masterCategory+gender\n\ndef getImageName(id): return '{name}.jpg'.format(name = id)\n\ndataFrame['image'] = dataFrame['id'].map(getImageName)\ndataFrame['target'] =dataFrame['usage']+dataFrame['gender']\n\ndataFrame.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nos quedamos solo con las columnas image y target\nds = dataFrame.filter(['image', 'target'])\nds.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets=ds.target.unique()\nCLASSES=len(targets)\nprint(CLASSES)\nprint(targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculamos la representacion que se obtiene para cada valor\nporcentajes={}\nfor target in targets:\n    perc = (((dataFrame['target'] == target)).sum() / dataFrame.shape[0]) * 100\n    porcentajes[target] = round(perc,4)\nprint(porcentajes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Master Category**","metadata":{}},{"cell_type":"code","source":"\ndataFrame['image'] = dataFrame['id'].map(getImageName)\ndataFrame['target'] =dataFrame['masterCategory']+dataFrame['gender']\nds = dataFrame.filter(['image', 'target'])\ntargets=ds.target.unique()\nCLASSES=len(targets)\nporcentajes={}\nfor target in targets:\n    perc = (((dataFrame['target'] == target)).sum() / dataFrame.shape[0]) * 100\n    porcentajes[target] = round(perc,4)\nprint(porcentajes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**subCategory**","metadata":{}},{"cell_type":"code","source":"\ndataFrame['image'] = dataFrame['id'].map(getImageName)\ndataFrame['target'] =dataFrame['subCategory']+dataFrame['gender']\nds = dataFrame.filter(['image', 'target'])\ntargets=ds.target.unique()\nCLASSES=len(targets)\nporcentajes={}\nfor target in targets:\n    perc = (((dataFrame['target'] == target)).sum() / dataFrame.shape[0]) * 100\n    porcentajes[target] = round(perc,4)\nprint(porcentajes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**articleType**","metadata":{}},{"cell_type":"code","source":"dataFrame['image'] = dataFrame['id'].map(getImageName)\ndataFrame['target'] =dataFrame['articleType']+dataFrame['gender']\nds = dataFrame.filter(['image', 'target'])\ntargets=ds.target.unique()\nCLASSES=len(targets)\nporcentajes={}\nfor target in targets:\n    perc = (((dataFrame['target'] == target)).sum() / dataFrame.shape[0]) * 100\n    porcentajes[target] = round(perc,4)\nprint(porcentajes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La única para la que se podrían obtener mejores resultados que con usage, es para la etiqueta mastercategory, por lo que copiaré este notebook y volveré a entrenar el sistema usando dicha etiqueta para comprobar si mejora el rendimiento.","metadata":{}},{"cell_type":"markdown","source":"UPDATE: No hay ninguna mejora significativa tras el cambio de etiqueta, por lo que este es el notebook final.","metadata":{}}]}